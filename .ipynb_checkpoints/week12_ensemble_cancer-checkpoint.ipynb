{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/Clair/machine_learning_sp20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# DATASET #1\n",
    "# Breast Cancer Tumor Analysis\n",
    "# -Classification\n",
    "#################################################################\n",
    "cancer_columns = [\"id_num\", \"diagnosis\", \"radius1\", \"texture1\", \"perimeter1\", \"area1\", \"smoothness1\", \n",
    "                  \"compactness1\", \"concavity1\", \"concave_pts1\", \"symmetry1\", \"fract_dimen1\",\n",
    "                 \"radius2\", \"texture2\", \"perimeter2\", \"area2\", \"smoothness2\", \n",
    "                  \"compactness2\", \"concavity2\", \"concave_pts2\", \"symmetry2\", \"fract_dimen2\",\n",
    "                 \"radius3\", \"texture3\", \"perimeter3\", \"area3\", \"smoothness3\", \n",
    "                  \"compactness3\", \"concavity3\", \"concave_pts3\", \"symmetry3\", \"fract_dimen3\"]\n",
    "cancer_data = pd.read_csv(\"wdbc.data\", delimiter=\",\", names=cancer_columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data[\"is_malig\"] = cancer_data.diagnosis.map({\"M\": 1, \"B\": 0})\n",
    "\n",
    "cancer_targets = cancer_data[\"is_malig\"]\n",
    "cancer_data = cancer_data.drop(columns=[\"id_num\", \"diagnosis\", \"is_malig\"])\n",
    "\n",
    "cancer_data = cancer_data[:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(cancer_data, cancer_targets,\n",
    "                                                                      stratify = cancer_targets,\n",
    "                                                                      test_size = 0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer with 5 inputs neurons\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=30, units=3, kernel_initializer=\"uniform\"))\n",
    "\n",
    "#Hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=7, kernel_initializer=\"uniform\"))\n",
    "\n",
    "classifier.add(Dense(activation=\"relu\", units=3, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "#output layer with 1 output neuron which will predict 1 or 0\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/20\n",
      "398/398 [==============================] - 0s 393us/step - loss: 0.6922 - accuracy: 0.6784 - val_loss: 0.6912 - val_accuracy: 0.6608\n",
      "Epoch 2/20\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.6902 - accuracy: 0.6709 - val_loss: 0.6886 - val_accuracy: 0.7076\n",
      "Epoch 3/20\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.6869 - accuracy: 0.7889 - val_loss: 0.6844 - val_accuracy: 0.8596\n",
      "Epoch 4/20\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.6808 - accuracy: 0.8744 - val_loss: 0.6763 - val_accuracy: 0.9181\n",
      "Epoch 5/20\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.6703 - accuracy: 0.8417 - val_loss: 0.6637 - val_accuracy: 0.7485\n",
      "Epoch 6/20\n",
      "398/398 [==============================] - 0s 80us/step - loss: 0.6545 - accuracy: 0.7412 - val_loss: 0.6439 - val_accuracy: 0.6082\n",
      "Epoch 7/20\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.6327 - accuracy: 0.6281 - val_loss: 0.6175 - val_accuracy: 0.8538\n",
      "Epoch 8/20\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.6053 - accuracy: 0.8543 - val_loss: 0.5802 - val_accuracy: 0.9123\n",
      "Epoch 9/20\n",
      "398/398 [==============================] - 0s 88us/step - loss: 0.5631 - accuracy: 0.8894 - val_loss: 0.5341 - val_accuracy: 0.9123\n",
      "Epoch 10/20\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.5153 - accuracy: 0.8970 - val_loss: 0.4786 - val_accuracy: 0.9240\n",
      "Epoch 11/20\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.4645 - accuracy: 0.8945 - val_loss: 0.4276 - val_accuracy: 0.9064\n",
      "Epoch 12/20\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.4145 - accuracy: 0.8769 - val_loss: 0.3694 - val_accuracy: 0.9181\n",
      "Epoch 13/20\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.3701 - accuracy: 0.8920 - val_loss: 0.3278 - val_accuracy: 0.9181\n",
      "Epoch 14/20\n",
      "398/398 [==============================] - 0s 63us/step - loss: 0.3380 - accuracy: 0.8945 - val_loss: 0.2966 - val_accuracy: 0.9181\n",
      "Epoch 15/20\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.3105 - accuracy: 0.8920 - val_loss: 0.2695 - val_accuracy: 0.9181\n",
      "Epoch 16/20\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.2925 - accuracy: 0.8869 - val_loss: 0.2567 - val_accuracy: 0.9123\n",
      "Epoch 17/20\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.2796 - accuracy: 0.8945 - val_loss: 0.2393 - val_accuracy: 0.9240\n",
      "Epoch 18/20\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.2688 - accuracy: 0.9045 - val_loss: 0.2299 - val_accuracy: 0.9181\n",
      "Epoch 19/20\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.2693 - accuracy: 0.9020 - val_loss: 0.2514 - val_accuracy: 0.9123\n",
      "Epoch 20/20\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.2609 - accuracy: 0.8920 - val_loss: 0.2192 - val_accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "ep = 20\n",
    "history = classifier.fit(train_data, train_targets,\n",
    "                    epochs=ep,\n",
    "                    validation_data=(test_data, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 20\n",
      "Test loss: 0.219, Test Accuracy: 0.924\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc = classifier.evaluate(test_data, test_targets, verbose=0)\n",
    "print(\"Number of epochs:\", ep)\n",
    "print('Test loss: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8236937061915889\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_data, train_targets);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_data)\n",
    "print(rf.score(test_data, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07 , 0.01 , 1.   , 1.   , 0.065, 0.9  , 0.005, 0.005, 0.02 ,\n",
       "       0.005, 0.705, 0.99 , 0.04 , 0.02 , 0.25 , 0.065, 0.   , 0.01 ,\n",
       "       0.335, 1.   , 1.   , 0.41 , 0.975, 0.   , 0.   , 0.   , 0.985,\n",
       "       0.   , 0.935, 0.06 , 0.135, 0.975, 0.   , 0.   , 0.   , 0.06 ,\n",
       "       0.   , 0.02 , 0.255, 0.085, 0.34 , 0.09 , 0.   , 0.48 , 0.085,\n",
       "       0.01 , 0.06 , 1.   , 0.035, 0.01 , 0.185, 0.26 , 0.125, 0.85 ,\n",
       "       0.985, 0.   , 0.005, 0.01 , 1.   , 0.   , 0.   , 0.89 , 0.01 ,\n",
       "       0.985, 0.975, 0.01 , 0.   , 0.46 , 0.   , 0.07 , 0.02 , 1.   ,\n",
       "       0.985, 0.005, 0.835, 1.   , 1.   , 0.86 , 0.   , 0.29 , 0.3  ,\n",
       "       0.015, 0.505, 0.005, 0.025, 0.015, 0.09 , 0.   , 0.02 , 0.995,\n",
       "       0.005, 0.   , 1.   , 0.945, 0.985, 0.68 , 0.015, 0.   , 0.04 ,\n",
       "       0.155, 1.   , 1.   , 1.   , 0.   , 0.03 , 0.035, 0.33 , 0.93 ,\n",
       "       0.06 , 0.   , 0.985, 0.005, 0.   , 1.   , 1.   , 0.905, 0.   ,\n",
       "       0.615, 1.   , 0.075, 0.01 , 0.985, 1.   , 1.   , 0.005, 0.025,\n",
       "       0.02 , 1.   , 0.   , 0.53 , 0.595, 0.01 , 1.   , 0.   , 0.015,\n",
       "       1.   , 0.015, 0.95 , 0.   , 0.815, 1.   , 0.93 , 0.075, 0.01 ,\n",
       "       0.235, 0.78 , 0.03 , 1.   , 0.01 , 0.31 , 1.   , 0.   , 0.9  ,\n",
       "       0.625, 0.165, 0.   , 0.945, 0.12 , 0.015, 0.26 , 0.4  , 0.625,\n",
       "       0.005, 0.   , 0.95 , 0.005, 0.785, 0.   , 0.15 , 0.   , 0.025])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8567425525700935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
